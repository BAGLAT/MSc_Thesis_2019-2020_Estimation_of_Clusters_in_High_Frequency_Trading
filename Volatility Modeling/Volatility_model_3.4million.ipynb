{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import lag, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (StructField,FloatType,TimestampType,StructType)\n",
    "from pyspark.sql import functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import format_number\n",
    "from arch import arch_model\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "plt.rcParams['agg.path.chunksize'] = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Start Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from S3 Bucket (SPY ETF Apr'19 - Jul'20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df = spark.read.csv(\"s3://aws-logs-664959780319-us-east-1/data/34lkh.csv\",header=True)\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Log Returns in 10^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df3 = df\n",
    "df3 = df3.withColumn(\"Id\", lit('1'))\n",
    "w = Window().partitionBy().orderBy(col(\"Id\"))\n",
    "df3 = df3.select(\"*\", lag(\"close\").over(w).alias(\"close_shift\"))\n",
    "df3 = df3.withColumn('returns', pyspark.sql.functions.log(col('close') / col('close_shift'))*1000)\n",
    "df3.show()\n",
    "#df3 = df3.select(df3['date'],df3['minute'],df3['close'],format_number(df3['returns'].cast('float'),6).alias('returns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df3 = df3.withColumn('sqd_returns_vol',df3['returns']**2)\n",
    "#df3 = df3.select(df3['date'],df3['minute'],df3['close'],df3['returns'],df3['cumulative_returns'],\n",
    "                #format_number(df3['sqd_returns_vol'].cast('float'),6).alias('sqd_returns_vol'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframe to Pandas Dataframe for Computing ARCH and GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df3 = df3.na.drop()\n",
    "df_vol = df3\n",
    "dfp = df_vol.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Modeling - ARCH(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "model_arch_1 = arch_model(dfp['returns'][1:],vol = \"ARCH\",p=1)\n",
    "results_arch_1 = model_arch_1.fit()\n",
    "results_arch_1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Modeling - GARCH (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "mod_garch_1 = arch_model(dfp.returns[1:], vol = \"GARCH\", p = 1, q=1)\n",
    "results_garch_1 = mod_garch_1.fit()\n",
    "results_garch_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing GARCH(1,1) Model on Test Data (Till End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dfp['datetime'] = pd.to_datetime(dfp.datetime,dayfirst = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dfp = dfp.set_index(dfp['datetime'])\n",
    "dfp = dfp.drop(['datetime'],axis=1)\n",
    "dfp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dfp['returns'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df_train = dfp.loc['2019-01-01 09:30:00':'2019-12-01 09:30:00']\n",
    "df_test = dfp.loc['2019-12-01 09:30:00':'2019-12-31 06:31:00']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining End Date for Volatility Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "start_date = \"2019-12-01 09:30:00\"\n",
    "end_date = \"2019-12-31 06:31:00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GARCH(1,1) Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "mod_garch_1 = arch_model(dfp.returns[1:], vol = \"GARCH\", p = 1,q=1)\n",
    "results_arch_1 = mod_garch_1.fit(last_obs = start_date)\n",
    "results_arch_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting 1 Period Ahead Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "pred_garch = results_arch_1.forecast(horizon=1)\n",
    "dff = pred_garch.residual_variance[start_date:end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dff['standardized_residuals'] = (dff['h.1'] - dff['h.1'].mean())/dff['h.1'].std()\n",
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Predicted 1 Minute to Daily Returns (Sum Intraday Squared Returns - Realized Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df_aggregated_to_daily_predicted = dff.resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df_aggregated_to_daily_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Actual 1 Minute to Daily Returns (Sum Intraday Squared Returns - Realized Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df_aggregated_to_daily_actual = df_test.resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual vs Predicted Volatility (Daily Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "t2['sqd_returns_vol'].plot(figsize = (15,12), color = \"green\")\n",
    "t1['h.1'].plot(figsize = (20,5), color = \"red\")\n",
    "plt.title(\"Actual Squared Returns vs Predicted Variance Residuals Aggregated to daily level\", size = 15)\n",
    "plt.xlabel(\"Date\",size=15)\n",
    "plt.ylabel(\"Variance (In range of 10^3)\",size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Normalized Residuals and Actual Squared Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dff['standardized_residuals'] = (dff['h.1'] - dff['h.1'].mean())/dff['h.1'].std()\n",
    "df_test['normalized_squarred_returns'] = (df_test['sqd_returns_vol'] - df_test['sqd_returns_vol'].mean())/df_test['sqd_returns_vol'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Standardized Squared Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dff['standardized_residuals'].plot(figsize = (20,5), color = \"red\")\n",
    "plt.title(\"Predicted standardized residuals\", size = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Standardized Squared Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df_test['normalized_squarred_returns'].plot(figsize = (20,5), color = \"orange\")\n",
    "plt.title(\"Actual normalized squarred returns\", size = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual vs Predicted Volatility (1-Minute Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df_test['normalized_squarred_returns'].plot(figsize = (20,5), color = \"green\")\n",
    "dff['standardized_residuals'].plot(figsize = (20,5), color = \"red\")\n",
    "plt.title(\"Actual Squared Returns vs Predicted Variance Residuals 1 Minute - Normalized\", size = 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Clock - Decision Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
